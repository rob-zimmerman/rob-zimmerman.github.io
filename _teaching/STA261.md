---
title: "STA261: Probability and Statistics II"
collection: teaching
type: "Undergraduate course"
permalink: /teaching/STA261
excerpt: "<p>Undergraduate course, <i>University of Toronto</i>, Summer 2020, Summer 2021, Summer 2022, Summer 2024 </p>

STA261 is intended to be a rigorous (but friendly) introduction to mathematical statistics for students in the Theory and Methods Statistics Specialist program at the University of Toronto. I taught this course four times, from 2020 to 2024."
venue: "University of Toronto"
date: 2020-07-01, 2021-07-01, 2022-07-01, 2024-07-01
location: "Toronto, ON, Canada"
---

STA261 is intended to be a rigorous (but friendly) introduction to mathematical statistics for students in the Theory and Methods Statistics Specialist program at the University of Toronto (UofT). I taught this course four times, from 2020 to 2024. The content here is mainly from the Summer 2024 offering.

## Syllabi

[Summer 2021](https://rob-zimmerman.github.io/files/teaching/STA261/summer_2021_syllabus.pdf) \
[Summer 2022](https://rob-zimmerman.github.io/files/teaching/STA261/summer_2022_syllabus.pdf) \
[Summer 2024](https://rob-zimmerman.github.io/files/teaching/STA261/summer_2024_syllabus.pdf)


## Modules (Summer 2024)

### Module 1: Statistics

#### Slides

[Module 1 - Statistics](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_1___statistics.pdf) \
[Module 1 - Statistics (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_1___statistics__annotated_.pdf)

#### Assignment

[Assignment 0](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_0.pdf) ([Solutions](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_0_solutions.pdf)) \
[Assignment 1](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_1.pdf)

#### Optional Readings and Resources

* [John D. Cook - Why heights are normally distributed](https://www.johndcook.com/blog/2008/07/20/why-heights-are-normally-distributed/)
* [John D. Cook - Why heights are not normally distributed](https://www.johndcook.com/blog/2008/07/20/why-heights-are-not-normally-distributed/)
* [John D. Cook - Insufficient Statistics](https://www.johndcook.com/blog/2016/09/12/insufficient-statistics/)
* [Dennis D. Boos and Jacqueline M. Hughes-Oliver - Applications of Basu's Theorem](http://homepages.math.uic.edu/~rgmartin/Teaching/Stat411/Refs/BoosOliver1998.pdf) (Just Section 3.2 on Monte Carlo swindles, which are very nifty)
* [StackExchange - Meaning of completeness of a statistic](https://stats.stackexchange.com/a/130904) (Another answer on the same page goes more into the linear algebra interpretation)



### Module 2: Point Estimation

#### Slides

[Module 2 - Statistics](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_2___point_estimation.pdf) \
[Module 2 - Point Estimation (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_2___point_estimation__annotated_.pdf)

#### Assignment

[Assignment 2](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_2.pdf)

#### Optional Readings and Resources

* [Psychology Wiki - Likelihood Principle](https://psychology.wikia.org/wiki/Likelihood_principle)
* [Cross Validated - When is a biased estimator preferable to an unbiased one?](https://stats.stackexchange.com/q/207760)
* [John Aldrich - R. A. Fisher and the Making of Maximum Likelihood 1912 - 1922](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Likelihood/Fisher%20and%20history%20of%20mle.pdf) (A wonderful minimal-math history/overview of the MLE, with further appearances by sufficiency and efficiency)
* [R. A. Fisher - Professor Karl Pearson and the Method of Moments](https://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1937.tb02149.x) and [Karl Pearson - Method of Moments and Method of Maximum Likelihood](https://www.jstor.org/stable/2334123) (Pearson essentially invented the MOM, while Fisher did the same for the MLE. Fisher and Pearson openly despised each other, and neither of them had warm personalities to begin with. In these two remarkable papers, Fisher and Pearson use the academic publication as a medium to publicly attack one another. Both papers are dripping with sarcasm and scorn, which you can easily get a sense of even if you skip past all the math.)
* [L. Bondesson - On Uniformly Minimum Variance Unbiased Estimation when no Complete Sufficient Statistics Exist](https://link.springer.com/content/pdf/10.1007/BF02056900.pdf)


### Module 3: Hypothesis Testing

#### Slides

[Module 3 - Hypothesis Testing](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_3___hypothesis_testing.pdf) \
[Module 3 - Hypothesis Testing (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_3___hypothesis_testing__annotated_.pdf)

#### Assignment

[Assignment 3](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_3.pdf)

#### Optional Readings and Resources

* [Shane Pederson - What Does a Lady Tasting Tea Have to Do with Science?](https://www.kdnuggets.com/2019/05/lady-tasting-tea-science.html)
* [Christie Aschwanden - Not Even Scientists Can Easily Explain P-Values](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/)
* [William Huber (whuber) - A Dialog [sic] Between a Teacher and a Thoughtful Student](https://stats.stackexchange.com/a/130772) (whuber is a well-known Cross Validated contributor/moderator)
* [Itai Yanai and Martin Lercher - A Hypothesis is a Liability](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02133-w) (This rather provocative article has generated much [discussion](https://statmodeling.stat.columbia.edu/2020/09/07/day-science-and-night-science-are-the-same-thing-if-done-right/) among [statisticians](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02276-4) since it was published)
* [Kristoffer Magnusson - Understanding Statistical Power and Significance Testing: an interactive visualization](https://rpsychologist.com/d3/nhst/) (An amazing visualization of the Z-test that lets you play around with various parameters of the test)
* [Raymond S. Nickerson - Null Hypothesis Significance Testing: A Review of an Old and Continuing Controversy](https://psycnet.apa.org/record/2000-07827-007)
* [John D. Cook - Are Coffee and Wine Good for You or Bad for You?](https://www.johndcook.com/blog/2018/01/24/are-coffee-and-wine-good-for-you-or-bad-for-you/)
* [George B. Dantzig - On the Non-Existence of Tests of "Student's" Hypothesis Having Power Functions Independent of Ïƒ](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-11/issue-2/On-the-Non-Existence-of-Tests-of-Students-Hypothesis-Having/10.1214/aoms/1177731912.full) (You ever hear the story of the university student who arrived to lecture late one day, saw two problems written on the blackboard that he assumed were homework problems and solved them before the next lecture, only to find out that they were actually unsolved problems? It's not an urban legend; the guy was a real person named George Dantzig, an outstanding mathematician who later went on to invent the simplex algorithm (if you ever did/do take a course in optimization, you'll learn about it), among many other things. The university class was a statistics course, and the professor teaching the lecture was Jerzy Neyman (he of the Neyman-Pearson lemma and much else). The problems were both conjectures related to hypothesis testing, and Neyman encouraged Dantzig to publish his proofs of the conjectures, which he did (and then stapled them together into what became his PhD thesis). The linked paper is the first of the two. After Module 3, you should have seen enough about hypothesis testing -- and -tests in particular -- to understand most everything up until the end of Section 2 of the paper, including the statement of the problem that Dantzig solved.)



### Module 4: Intervals and Model Checking

#### Slides

[Module 4 - Intervals and Model Checking](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_4___intervals_and_model_checking.pdf) \
[Module 4 - Intervals and Model Checking (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_4___intervals_and_model_checking__annotated_.pdf)

#### Assignment

[Assignment 4](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_4.pdf)

#### Optional Readings and Resources

* [Hoekstra et al. - Robust Misinterpretation of Confidence Intervals](https://link.springer.com/article/10.3758/s13423-013-0572-3)
* [Kristoffer Magnusson - Interpreting Confidence Intervals](https://rpsychologist.com/d3/ci/) (Another excellent visualization from the same author as the Module 3 visualization for hypothesis testing)
* [Wikipedia - All Models Are Wrong](https://en.wikipedia.org/wiki/All_models_are_wrong) (George Box's quote is probably the most famous aphorism in Statistics)
* [Vincent Scheurer - Convicted on Statistics?](https://understandinguncertainty.org/node/545) (A tragic case study on the dangers of poor model assumptions. This is a difficult read.)
* [StackExchange - How to Understand Degrees of Freedom?](https://stats.stackexchange.com/q/16921)
* [Wikipedia - Chernoff Face](https://en.wikipedia.org/wiki/Chernoff_face) (In lecture, I mentioned that visual assessments are useful because the human eye is much better at noticing deviations from expected visuals than computers are. Chernoff faces are a very cool way of exploiting this to visualize different sets of multivariate observations)

#### When Uniform Numbers Aren't That Uniform...

```r
library(rgl) # for plotting

####

# A linear congruential generator: starting with an initial X_{0}, we recursively update X_{i+1} = c + a*X{i} (mod m) 
# for some fixed c, prime number a, m and then return (X_{0}, ..., X_{n})/m

####

a <- 65539
m <- 2^31
seed <- 1234

lcg <- function(n, seed, a, m) {
  X <- integer(n)
  X[1] <- seed
  for (i in 2:n) {
    prod = as.numeric(X[i-1]) * a
    X[i] <- as.integer(prod %% m)
  }
  return(X / m)
}

n <- 10000
X <- lcg(n, seed, a, m)

plot(X, xlab = 'i', ylab = 'X[i]', col = 'blue')

plot(X[1:(n-1)], X[2:n], xlab = 'X[i]', ylab = 'X[i+1]', col = 'blue')

rgl::plot3d(X[1:(n-2)], X[2:(n-1)], X[3:n],
            xlab = 'X[i]', ylab = 'X[i+1]', zlab = 'X[i+2]', col = 'blue')
```



### Module 5: Asymptotic Extensions

#### Slides

[Module 5 - Asymptotic Extensions](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_5___asymptotic_extensions.pdf) \
[Module 5 - Asymptotic Extensions (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_5___asymptotic_extensions__annotated_.pdf)

#### Assignment

[Assignment 5](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_5.pdf)

#### Optional Readings and Resources

* [StackExchange - Large sample asymptotic/theory - Why to care about?](https://stats.stackexchange.com/q/58131)
* [A. Buse - The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note](https://www.jstor.org/stable/2683166)
* [Alan Agresti and Brent A. Coull - Approximate Is Better than "Exact" for Interval Estimation of Binomial Proportions](https://www.jstor.org/stable/2685469)
* [Brad Efron and David E. Hinkley - Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information](https://www.jstor.org/stable/2335893) (The math gets pretty heavy after Section 1, but you can still skim through the empirical results)
* [Aad van der Vaart - Superefficiency](http://www.stat.yale.edu/~pollard/Books/LeCamFest/VanderVaart.pdf) (Just Section 27.1, for the story of Hodges' estimator that appears in the assignment.)

#### Asymptotics in Action

If you know some basic R and you've worked through the assignment, try to figure out what this does and then run it. If you aren't amazed, you should be!

```r
set.seed(123456)
wald <- 0
score <- 0
J <- 1000
N <- 10000

for (j in 1:J) {
    dat <- rnorm(n=N, mean=10, sd=sqrt(4))
    vardat <- (1/N)*sum((dat-10)^2)

    L_w <- (1-sqrt(2/N)*1.96)*vardat
    U_w <- (1+sqrt(2/N)*1.96)*vardat
    if (L_w < 4 && 4 < U_w) {
    wald <- wald + 1
    }

    L_s <- vardat/(1+sqrt(2/N)*1.96)
    U_s <- vardat/(1-sqrt(2/N)*1.96)
    if (L_s < 4 && 4 < U_s) {
    score <- score + 1
    }
}

score_conf <- score/J
wald_conf <- wald/J
```



### Module 6: Bayesian Statistics

#### Slides

[Module 6 - Bayesian Statistics](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_6___bayesian_statistics.pdf) \
[Module 6 - Bayesian Statistics (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/2024_slides/module_6___bayesian_statistics__annotated_.pdf)

#### Assignment

[Assignment 6](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_6.pdf)

#### Optional Readings and Resources

* [StackExchange - Bayesian and Frequentist Reasoning in Plain English](https://stats.stackexchange.com/q/22)
* [StackExchange - Interpret Bayesian Probability as Frequentist Probability](https://philosophy.stackexchange.com/q/8275)
* [Jan Springer - The Objectivity of Subjective Bayesianism](https://philpapers.org/rec/SPRTOO-4)
* [Wicaksono Wijono - Don't Use Uniform Priors](https://towardsdatascience.com/stop-using-uniform-priors-47473bdd0b8a)
* [Dan Simpson - Asymptotically we are all dead](https://statmodeling.stat.columbia.edu/2017/11/27/asymptotically-we-are-all-dead/)
* [Radford M. Neal - Philosophy of Bayesian Inference](https://www.cs.toronto.edu/~radford/res-bayes-ex.html) (Radford is professor emeritus in our department)
* [Andrew Gelman - Bayesian Statistical Pragmatism](https://statmodeling.stat.columbia.edu/2011/04/15/bayesian_statis_1/)
* [Daniel Kunin - Seeing Theory (Chapter 3: Bayesian Inference)](https://seeing-theory.brown.edu/bayesian-inference/index.html)



## Assessments 

### Summer 2021

[Summer 2021 - Quiz 1](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_1.pdf) \
[Summer 2021 - Quiz 2](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_2.pdf) \
[Summer 2021 - Quiz 3](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_3.pdf) \
[Summer 2021 - Quiz 4](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_4.pdf) \
[Summer 2021 - Quiz 5](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_5.pdf) \
[Summer 2021 - Final Assessment](https://rob-zimmerman.github.io/files/teaching/STA261/2021_final_assessment.pdf)

### Summer 2022 

[Summer 2022 - Midterm 1](https://rob-zimmerman.github.io/files/teaching/STA261/2022_midterm_1.pdf) \
[Summer 2022 - Midterm 2](https://rob-zimmerman.github.io/files/teaching/STA261/2022_midterm_2.pdf) \
[Summer 2022 - Final Exam](https://rob-zimmerman.github.io/files/teaching/STA261/2022_final_examination.pdf)

### Summer 2024

[Summer 2024 - Midterm 1](https://rob-zimmerman.github.io/files/teaching/STA261/2024_midterm_1.pdf) \
[Summer 2024 - Midterm 2](https://rob-zimmerman.github.io/files/teaching/STA261/2024_midterm_2.pdf) \
[Summer 2024 - Final Exam](https://rob-zimmerman.github.io/files/teaching/STA261/2024_final_examination.pdf)