---
title: "STA261: Probability and Statistics II"
collection: teaching
type: "Undergraduate course"
permalink: /teaching/STA261
excerpt: "<p>Undergraduate course, <i>University of Toronto</i>, Summer 2020, Summer 2021, Summer 2022 </p>

STA261 is intended to be a rigorous (but friendly) introduction to mathematical statistics for students in the Theory and Methods Statistics Specialist program at the University of Toronto. I taught this course three times, from 2020 to 2022."
venue: "University of Toronto"
date: 2020-07-01, 2021-07-01
location: "Toronto, ON, Canada"
---

STA261 is intended to be a rigorous (but friendly) introduction to mathematical statistics for students in the Theory and Methods Statistics Specialist program at the University of Toronto (UofT). I taught this course three times, from 2020 to 2022. The content here is mainly from the Summer 2022 offering.

## Syllabi

[Summer 2021](https://rob-zimmerman.github.io/files/teaching/STA261/summer_2021_syllabus.pdf) \
[Summer 2022](https://rob-zimmerman.github.io/files/teaching/STA261/summer_2022_syllabus.pdf)


## Modules (Summer 2022)

### Module 1: Statistics

#### Slides

[Module 1 - Statistics](https://rob-zimmerman.github.io/files/teaching/STA261/module_1___statistics.pdf) \
[Module 1 - Statistics (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/module_1___statistics__annotated_.pdf)

#### Assignment

[Assignment 0](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_0.pdf) (This was meant to be a probability warm-up before the course began) \
[Assignment 1](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_1.pdf)

#### Optional Readings and Resources

* [John D. Cook - Insufficient Statistics](https://www.johndcook.com/blog/2016/09/12/insufficient-statistics/)
* [Dennis D. Boos and Jacqueline M. Hughes-Oliver - Applications of Basu's Theorem](http://homepages.math.uic.edu/~rgmartin/Teaching/Stat411/Refs/BoosOliver1998.pdf) (Just Section 3.2 on Monte Carlo swindles, which are very nifty)
* [StackExchange - Meaning of completeness of a statistic](https://stats.stackexchange.com/a/130904) (Another answer on the same page goes more into the linear algebra interpretation)



### Module 2: Point Estimation

#### Slides

[Module 2 - Statistics](https://rob-zimmerman.github.io/files/teaching/STA261/module_2___point_estimation.pdf) \
[Module 2 - Point Estimation (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/module_2___point_estimation__annotated_.pdf)

#### Assignment

[Assignment 2](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_2.pdf)

#### Optional Readings and Resources

* [Psychology Wiki - Likelihood Principle](https://psychology.wikia.org/wiki/Likelihood_principle)
* [Cross Validated - When is a biased estimator preferable to an unbiased one?](https://stats.stackexchange.com/q/207760)
* [John Aldrich - R. A. Fisher and the Making of Maximum Likelihood 1912 - 1922](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Likelihood/Fisher%20and%20history%20of%20mle.pdf) (A wonderful minimal-math history/overview of the MLE, with further appearances by sufficiency and efficiency)
* [R. A. Fisher - Professor Karl Pearson and the Method of Moments](https://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1937.tb02149.x) and [Karl Pearson - Method of Moments and Method of Maximum Likelihood](https://www.jstor.org/stable/2334123) (Pearson essentially invented the MOM, while Fisher did the same for the MLE. Fisher and Pearson openly despised each other, and neither of them had warm personalities to begin with. In these two remarkable papers, Fisher and Pearson use the academic publication as a medium to publicly attack one another. Both papers are dripping with sarcasm and scorn, which you can easily get a sense of even if you skip past all the math.)
* [L. Bondesson - On Uniformly Minimum Variance Unbiased Estimation when no Complete Sufficient Statistics Exist](https://link.springer.com/content/pdf/10.1007/BF02056900.pdf)


### Module 3: Hypothesis Testing

#### Slides

[Module 3 - Hypothesis Testing](https://rob-zimmerman.github.io/files/teaching/STA261/module_3___hypothesis_testing.pdf) \
[Module 3 - Hypothesis Testing (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/module_3___hypothesis_testing__annotated_.pdf)

#### Assignment

[Assignment 3](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_3.pdf)

#### Optional Readings and Resources

* [Shane Pederson - What Does a Lady Tasting Tea Have to Do with Science?](https://www.kdnuggets.com/2019/05/lady-tasting-tea-science.html)
* [Christie Aschwanden - Not Even Scientists Can Easily Explain P-Values](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/)
* [William Huber (whuber) - A Dialog Between a Teacher and a Thoughtful Student](https://stats.stackexchange.com/a/130772) (whuber is a well-known Cross Validated contributor/moderator)
* [Itai Yanai and Martin Lercher - A Hypothesis is a Liability](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02133-w) (This rather provocative article has generated much [discussion](https://statmodeling.stat.columbia.edu/2020/09/07/day-science-and-night-science-are-the-same-thing-if-done-right/) among [statisticians](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02276-4) since it was published)
* [Kristoffer Magnusson - Understanding Statistical Power and Significance Testing: an interactive visualization](https://rpsychologist.com/d3/nhst/) (An amazing visualization of the Z-test that lets you play around with various parameters of the test)
* [Raymond S. Nickerson - Null Hypothesis Significance Testing: A Review of an Old and Continuing Controversy](https://psycnet.apa.org/record/2000-07827-007)
* [John D. Cook - Are Coffee and Wine Good for You or Bad for You?](https://www.johndcook.com/blog/2018/01/24/are-coffee-and-wine-good-for-you-or-bad-for-you/)



### Module 4: Intervals and Model Checking

#### Slides

[Module 4 - Intervals and Model Checking](https://rob-zimmerman.github.io/files/teaching/STA261/module_4___intervals_and_model_checking.pdf) \
[Module 4 - Intervals and Model Checking (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/module_4___intervals_and_model_checking__annotated_.pdf)

#### Assignment

[Assignment 4](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_4.pdf)

#### Optional Readings and Resources

* [Hoekstra et al. - Robust Misinterpretation of Confidence Intervals](https://link.springer.com/article/10.3758/s13423-013-0572-3)
* [Kristoffer Magnusson - Interpreting Confidence Intervals](https://rpsychologist.com/d3/ci/) (Another excellent visualization from the same author as the Module 3 visualization for hypothesis testing)
* [Wikipedia - All Models Are Wrong](https://en.wikipedia.org/wiki/All_models_are_wrong) (George Box's quote is probably the most famous aphorism in Statistics)
* [Vincent Scheurer - Convicted on Statistics?](https://understandinguncertainty.org/node/545) (A tragic case study on the dangers of poor model assumptions. This is a difficult read.)
* [StackExchange - How to Understand Degrees of Freedom?](https://stats.stackexchange.com/q/16921)
* [Wikipedia - Chernoff Face](https://en.wikipedia.org/wiki/Chernoff_face) (In lecture, I mentioned that visual assessments are useful because the human eye is much better at noticing deviations from expected visuals than computers are. Chernoff faces are a very cool way of exploiting this to visualize different sets of multivariate observations)



### Module 5: Asymptotic Extensions

#### Slides

[Module 5 - Asymptotic Extensions](https://rob-zimmerman.github.io/files/teaching/STA261/module_5___asymptotic_extensions.pdf) \
[Module 5 - Asymptotic Extensions (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/module_5___asymptotic_extensions__annotated_.pdf)

#### Assignment

[Assignment 5](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_5.pdf)

#### Optional Readings and Resources

* [StackExchange - Large sample asymptotic/theory - Why to care about?](https://stats.stackexchange.com/q/58131)
* [A. Buse - The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note](https://www.jstor.org/stable/2683166)
* [Alan Agresti and Brent A. Coull - Approximate Is Better than "Exact" for Interval Estimation of Binomial Proportions](https://www.jstor.org/stable/2685469)
* [Brad Efron and David E. Hinkley - Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information](https://www.jstor.org/stable/2335893) (The math gets pretty heavy after Section 1, but you can still skim through the empirical results)
* [Aad van der Vaart - Superefficiency](http://www.stat.yale.edu/~pollard/Books/LeCamFest/VanderVaart.pdf) (Just Section 27.1, for the story of Hodges' estimator that appears in the assignment.)

#### Asymptotics in Action

If you know some basic R and you've worked through the assignment, try to figure out what this does and then run it. If you aren't amazed, you should be!

```r
set.seed(123456)
wald <- 0
score <- 0
J <- 1000
N <- 10000

for (j in 1:J) {
    dat <- rnorm(n=N, mean=10, sd=sqrt(4))
    vardat <- (1/N)*sum((dat-10)^2)

    L_w <- (1-sqrt(2/N)*1.96)*vardat
    U_w <- (1+sqrt(2/N)*1.96)*vardat
    if (L_w < 4 && 4 < U_w) {
    wald <- wald + 1
    }

    L_s <- vardat/(1+sqrt(2/N)*1.96)
    U_s <- vardat/(1-sqrt(2/N)*1.96)
    if (L_s < 4 && 4 < U_s) {
    score <- score + 1
    }
}

score_conf <- score/J
wald_conf <- wald/J
```



### Module 6: Bayesian Statistics

#### Slides

[Module 6 - Bayesian Statistics](https://rob-zimmerman.github.io/files/teaching/STA261/module_6___bayesian_statistics.pdf) \
[Module 6 - Bayesian Statistics (Annotated)](https://rob-zimmerman.github.io/files/teaching/STA261/module_4___bayesian_statistics__annotated_.pdf)

#### Assignment

[Assignment 6](https://rob-zimmerman.github.io/files/teaching/STA261/assignment_6.pdf)

#### Optional Readings and Resources

* [StackExchange - Bayesian and Frequentist Reasoning in Plain English](https://stats.stackexchange.com/q/22)
* [StackExchange - Interpret Bayesian Probability as Frequentist Probability](https://philosophy.stackexchange.com/q/8275)
* [Jan Springer - The Objectivity of Subjective Bayesianism](https://philpapers.org/rec/SPRTOO-4)
* [Wicaksono Wijono - Don't Use Uniform Priors](https://towardsdatascience.com/stop-using-uniform-priors-47473bdd0b8a)
* [Dan Simpson - Asymptotically we are all dead](https://statmodeling.stat.columbia.edu/2017/11/27/asymptotically-we-are-all-dead/)
* [Radford M. Neal - Philosophy of Bayesian Inference](https://www.cs.toronto.edu/~radford/res-bayes-ex.html) (Radford is professor emeritus in our department)
* [Andrew Gelman - Bayesian Statistical Pragmatism](https://statmodeling.stat.columbia.edu/2011/04/15/bayesian_statis_1/)
* [Daniel Kunin - Seeing Theory (Chapter 3: Bayesian Inference)](https://seeing-theory.brown.edu/bayesian-inference/index.html)



## Assessments 

### Summer 2021

[Summer 2021 - Quiz 1](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_1.pdf) \
[Summer 2021 - Quiz 2](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_2.pdf) \
[Summer 2021 - Quiz 3](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_3.pdf) \
[Summer 2021 - Quiz 4](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_4.pdf) \
[Summer 2021 - Quiz 5](https://rob-zimmerman.github.io/files/teaching/STA261/2021_quiz_5.pdf) \
[Summer 2021 - Final Assessment](https://rob-zimmerman.github.io/files/teaching/STA261/2021_final_assessment.pdf)

### Summer 2022 

[Summer 2022 - Midterm 1](https://rob-zimmerman.github.io/files/teaching/STA261/2022_midterm_1.pdf) \
[Summer 2022 - Midterm 2](https://rob-zimmerman.github.io/files/teaching/STA261/2022_midterm_2.pdf) \
[Summer 2022 - Final Exam](https://rob-zimmerman.github.io/files/teaching/STA261/2022_final_examination.pdf)